---
title: "Final Project - Curated Bladder Data"
author: "John Kidd, Teeranan(Ben) Pokaprakarn, Sarah Reifeis"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
---

Our group topic is cancer genomics, and we worked with the curatedBladderData data set for this project, specifically attempting to mimic results from this [paper](https://www.ncbi.nlm.nih.gov/pubmed/15930339). To give you a little background on why we care about bladder cancer, we have some info below from the American Cancer Society to bring it into focus.

# Context for Bladder Cancer
The [American Cancer Society](https://www.cancer.org/cancer/bladder-cancer/about/key-statistics.html) estimates that there will be around 80,000 new cases of bladder cancer in the US in 2017, constituting approximately 5% of all new cancer cases. Of these, men are expected to be affected at 3 times the rate of women, and there are about 17,000 total deaths expected.

Bladder cancer tumors can be divided into two groups: superficial and muscle-invasive. This relates to staging of cancer (0/a, 1, 2, 3, 4) as follows. Generally, the 0 or a stage & 1 are considered superficial tumors because they haven't yet invaded the muscular wall of the bladder; stage 2 is characterized by the tumor invading the muscle wall of the bladder, and so stage 2 and beyond are classified as muscle-invasive tumors. Around half of all bladder cancers are first detected before becoming muscle-invasive. 

# Background on Our Data
The particular data set we chose to use in the curatedBladderData collection contains gene expression of 6225 genes for each of 80 subjects with bladder tumors. Some covariates of interest that we make use of later on are tumor subtype (superficial, muscle-invasive) and tumor stage. 

As we began working with the data set, it became apparent that there was an abundance of missing data for gene expression. In figuring out how to deal with this, we plotted gene expression standard deviation against the number of subjects with missing values for each gene. 

```{r echo = FALSE,message=FALSE, warning=FALSE}
library(curatedBladderData)
library(dendextend)
library(RColorBrewer)
library(matrixStats)
library(caret)
data("GSE1827_eset")
a1 = GSE1827_eset
eSet = exprs(a1)
plot(sqrt(rowVars(eSet, na.rm=TRUE)), rowSums(is.na(eSet)), xlab="Standard deviation",ylab="Number of subjects with missing values for each gene")

```

As you can see, there is still a lot of variance in some genes with a lot of missing data, so we don't want to throw this info away. Hence, we made the decision to use median imputation to impute gene expression across genes for the remainder of our project. 

The 80 bladder tumors were obtained frozen from the tissue bank of the UCSF Comprehensive Cancer Center, and the tumors were staged according to professionals from the American Joint Committee on Cancer. Of note, the paper mentioned above claims that the 80 tumors can be broken up into 17 stage a, 10 stage 1, 15 stage 2, 25 stage 3, and 13 stage 4. However, upon examining the data ourselves, we believe this to be a typo in the paper or there may be an error present in the data set. From the data set we used, we have that there are 17 stage a, 10 stage 1, 14 stage 2, 26 stage 3, and 13 stage 4, and this is the data we will use for the remainder of our project.

# Roadmap
We have 3 main objectives that we will cover throughout this project:

Firstly, we attempt to recreate a cluster dendrogram to resemble Fig 1 in the paper mentioned above. Several methods for clustering will be explored and compared to Fig 1 of the paper. 

Secondly, we investigate differences in gene expression levels for superficial and muscle-invasive tumors using heatmaps. In the paper, they created a heatmap (Fig 2) using an unspecified subset of around 2k genes (same set they used to create their cluster dendrogram). We didn't have a way to find out what these genes were, so we created several of our own subsets and generated heatmaps and compared them. 

Thirdly, we fit our subsets of genes using various methods to predict, based on expression, whether each tumor should be classified as superficial or muscle-invasive. We then used 5-fold cross-validation to obtain a mis-classification rate for each combination of fitting method and gene subset. To try and get a more accurate measurement of the true mis-classification rate, we run 10 cross-validations and take the average error across the 10 iterations. 

## Preprocessing done by the paper

1. Signal intensity values of each element were extracted using the GenePix software.

2. Intensities were then corrected by subarray median centering followed by global locally weighted scatterplot smoothing correction

3. Values are normalized and median centered

#Clusters
In this section, we attempt to recreate Figure 1 in the paper with a few differences described below.

```{r, echo=FALSE}
library(knitr)
knitr:: include_graphics("https://github.com/jkidd20/CompBioProject/blob/master/figure1_paper.png?raw=true")
```


### Clustering method from the paper
1. Filtering of genes: log2 ratios were filtered to include only these clones showing expression ratios in at least 80% of samples analyzed
Only clones that had |log2 ratio|>2 in at least one sample were used

2. Unsupervised hierarchical clustering

3. Linkage method: Average linkage (join clusters whose avg distance are the smallest)

4. Distance/Similarity matrix: uncentered Pearson correlation metric

5. Bottom-up clustering direction

### Justification for our method
We cannot perform the filtering as described by the paper, hence we chose a few other ways from class we can try to reduce the number of dimensions. We also explore other methods of clustering in order to examine if the choices about the clustering algorithm with regards to imputation, filtering, and clustering method made by the author make a difference in the validity of the result. In other words, we examine if the validity of the claim in the paper is sensitive to these choices.

### What we did

We try three different approaches of clustering with different choices of how we select the genes to include, and the imputation. We also use a slightly different clustering algorithm from the paper. We chose hierarchical clustering with ward's method.

##### First approach: 
Filtering: we just select top 100 genes with highest variance (before imputing); 

Imputation: Median impute

##### Second approach:
Non-missing genes only (621 genes left); 

10 principal components

##### Third approach

Median Impute ; 500 random genes

## Cluster with the top 100 genes with the highest variance


```{r, echo = FALSE, message=FALSE, warning=FALSE}
set.seed(1)
rv = rowVars(eSet, na.rm = TRUE)
o = order(rv, decreasing = TRUE)
imputedV = preProcess(t(eSet), method = "medianImpute")
eSet.imputed = t(predict(imputedV, t(eSet)))

dists2 = dist(t(eSet.imputed[head(o, 100), ]))

palette(brewer.pal(8, "RdBu"))
hc = hclust(dists2, method = "ward.D2")
#how does it cluster with missing value
library(dendextend)
dend <- as.dendrogram(hc)
o.dend <- order.dendrogram(dend)
labels <- paste(colnames(a1)[o.dend], pData(a1)$summarystage[o.dend])
#labels(dend) <- labels
#labels(dend) <- pData(a1)$T[o.dend]
#labels_colors(dend) <- 1*(pData(a1)$summarystage[o.dend] == "superficial") + 2
labels(dend) <- paste(substr(colnames(a1)[o.dend],4,8) , substr(pData(a1)$histological_type[o.dend],1,3))
colors <- rep(NA, length(pData(a1)$T))
ordered.T <- pData(a1)$T[o.dend]
colors[ordered.T < 2] <- ordered.T[ordered.T < 2] +1 
colors[ordered.T >= 2] <- ordered.T[ordered.T >= 2]+4
colors.bar <- rep(NA, length(pData(a1)$summarystage))
ordered.stage <- pData(a1)$summarystage[o.dend]
colors.bar[ordered.stage == "superficial"] <- "red"
colors.bar[ordered.stage == "invasive"] <- "blue"
labels_colors(dend) <- colors
plot(dend, horiz = FALSE)
title(main = "Hclust with top 100 genes with highest variance")
legend("topleft", legend = 0:4, fill = sort(unique(colors)))
colored_bars(colors.bar, dend, rowLabels = "Tumor type", sort_by_labels_order = FALSE)
```

Number 0 to 4 denote the stage of tumor. The red color in the colored bar at the bottom denotes superficial tumor (stage 0 and stage 1) while blue denotes the muscle invasive tumor. 'tcc' and 'squ' refers to the histological type.

```{r, echo = FALSE, message=FALSE, warning=FALSE}

cluster <- cutree(hc, k=2)
table(cluster, pData(a1)$summarystage)
table(cluster, pData(a1)$histological_type)
table(cluster, pData(a1)$T)
```
We see here that the two clusters are distinct in terms of the proportion of superficial and muscle-invasive tumor. we only have 6 misclassifications (5 in superficial and 1 in muscle invasive). Note also that all the 'squamous' histological type tumor are in the same cluster. This is consistent with the results reported by the paper which also have 6 similar misclassifications. This shows that at least in this example, the result is not sensitive to the choice of filtering, imputation, and clustering algorithm. We can still clearly distinguish the superficial and the muscle invasive tumor based on the unsupervised cluster.

## Clustering with only 10 Principal Components
```{r , echo = FALSE, message=FALSE, warning=FALSE}
set.seed(1)
numMis = rowSums(is.na(eSet))
eset.nonmissing <- eSet[numMis == 0, ]
#Only have 621 gene left
pc <- prcomp(t(eset.nonmissing))
dists.pc = dist(pc$x[,1:10])
palette(brewer.pal(8, "RdBu"))
hc = hclust(dists.pc, method = "ward.D2")
dend <- as.dendrogram(hc)
o.dend <- order.dendrogram(dend)
labels <- paste(colnames(a1)[o.dend], pData(a1)$summarystage[o.dend])
labels(dend) <- paste(substr(colnames(a1)[o.dend],4,8) , substr(pData(a1)$histological_type[o.dend],1,3))
colors <- rep(NA, length(pData(a1)$T))
ordered.T <- pData(a1)$T[o.dend]
colors[ordered.T < 2] <- ordered.T[ordered.T < 2] +1 
colors[ordered.T >= 2] <- ordered.T[ordered.T >= 2]+4
colors.bar <- rep(NA, length(pData(a1)$summarystage))
ordered.stage <- pData(a1)$summarystage[o.dend]
colors.bar[ordered.stage == "superficial"] <- "red"
colors.bar[ordered.stage != "superficial"] <- "blue"
labels_colors(dend) <- colors
plot(dend, horiz = FALSE)
title(main = "Hclust with top 10 principal components")
legend("topleft", legend = 0:4, fill = sort(unique(colors)))
colored_bars(colors.bar, dend, rowLabels = "Tumor type", sort_by_labels_order = FALSE)

```

```{r, echo = FALSE}
cluster <- cutree(hc, k=2)
table(cluster, pData(a1)$summarystage)
```

Here we also have 6 misclassifications (4 in superficial and 2 in muscle invasive). This shows that even with just 10 principle components based on the 621 non-missing genes compared to the 2,675 genes the paper uses after filtering, we can still have two distinct clusters with the same misclassifications.

## Clustering with 500 random genes
```{r, echo = FALSE, message=FALSE, warning=FALSE}
set.seed(1)
eSet2 = eSet.imputed
num.genes <- dim(eSet2)[1]
error.list <- c()
for (i in 1:100){
  sampled.genes <- sample(1:num.genes, size=500, replace = FALSE, prob = NULL)
  eSet2.random <- eSet2[sampled.genes, ]
  dists.random = dist(t(eSet2.random))
  hc = hclust(dists.random, method = "ward.D2")
  #how does it cluster with missing value
  dend <- as.dendrogram(hc)
  o.dend <- order.dendrogram(dend)
  labels(dend) <- colnames(a1)[o.dend]
  labels_colors(dend) <- 1*(pData(a1)$summarystage[o.dend] == "superficial") + 2
  cluster <- cutree(hc, k=2)
  conf.matrix <- table(cluster, pData(a1)$summarystage)
  error <- conf.matrix[1,2]+conf.matrix[2,1]
  error.list <- c(error.list,error)
}
palette(brewer.pal(8, "Dark2"))
plot(dend)
title(main = "Hclust with top 500 random genes")
```

An example of a hierarchical cluster of 500 random genes. We can still see that there are two distinct clusters.

```{r, echo = FALSE, message=FALSE, warning=FALSE}
hist(error.list,freq=TRUE,main="Histogram of Number of Misclassifications")
```

The histogram shows that 80 percent of the times when we randomly select 500 genes, we still obtain less than 10 misclassifications. This shows that there might be some batch effect across the genes between the two types of tumors because even when we randomly pick genes, we can still differentiate the two types of tumor. Another possible explanation is that there are signals in a lot of these genes and therefore even when we randomly pick, we are still getting some signals.

# Heatmaps
```{r setup, echo = FALSE, message=FALSE, warning=FALSE, eval=FALSE}
set.seed(784)
#Functions then calls
library(curatedBladderData)
library(limma)
library(caret)
library(e1071)
library(pheatmap)
library(RColorBrewer)

geneCV = function(expData, label, pCut = .000001, nFold = 5, fitMethod = "rf", impMethod = "medianImpute", 
                  randomFlag = FALSE, nSelect = NA, nrep = 10){
    eSet.t = exprs(expData)
    label = as.factor(label)
    labLev = levels(label)
    g1Num = which(label == labLev[1])
    g2Num = which(label == labLev[2])
    
    ### Imputation
    imputedV = preProcess(t(eSet.t), method = impMethod)
    eSet = t(predict(imputedV, t(eSet.t)))
    
    retValue = list()
    for(repNum in seq_len(nrep)){
      ### Cross Validation
      toSample1 = matrix(1:nFold, 1, length(g1Num))
      cvNum1 = sample(toSample1)
      toSample2 = matrix(1:nFold, 1, length(g2Num))
      cvNum2 = sample(toSample2)
      
      numWrong = rep(0, nFold)
      genes = list()
      
      for(i in 1:nFold){
        # set up Train and Test set
        trainLabels = c(rep(labLev[1], sum(cvNum1 != i)), rep(labLev[2], sum(cvNum2 != i)))
        testLabels = c(rep(labLev[1], sum(cvNum1 == i)), rep(labLev[2], sum(cvNum2 == i)))
        
        trainExprs = rbind(t(eSet)[g1Num[cvNum1 != i], ], t(eSet)[g2Num[cvNum2 != i], ])
        testExprs = rbind(t(eSet)[g1Num[cvNum1 == i], ], t(eSet)[g2Num[cvNum2 == i], ])
        
        
        trainDF = data.frame(trainLabels, trainExprs)
        testDF = data.frame(testLabels, testExprs)
        
        colnames(trainDF)[1] = colnames(testDF)[1] = "label"
        
        # train data
        if(!randomFlag){
          design <- model.matrix(~ trainDF[, 1])
          fit <- lmFit(t(trainExprs), design)
          fit <- eBayes(fit)
          tab <- topTable(fit, n=nrow(eSet), sort.by="none")
          if(is.na(nSelect)){
            genes[[i]] = gsub('-', '.', rownames(tab)[tab$adj.P.Val < pCut])
          }
          else{
            genes[[i]] = gsub('-', '.', rownames(tab[order(tab$adj.P.Val),])[1:nSelect])
          }
        }
        else{
          genes[[i]] = gsub('-', '.', sample(rownames(expData), nSelect))
        }
        
        smallTrain = cbind(trainDF[, 1], trainDF[, colnames(trainDF) %in% genes[[i]]])
        colnames(smallTrain)[1] = "label"
        
        mFit = train(label ~ ., data = smallTrain, method = fitMethod)
        
        # Predict on Test
        
        predValues = predict(mFit, testDF)
        numWrong[i] = sum(as.character(predValues) != as.character(testDF$label))
      }
      retValue[[repNum]] = list(genes = genes, cvError = mean(numWrong / table(c(cvNum1, cvNum2))), 
                       impMethod = impMethod, label = label)
    }
    return(retValue)
}

aveCVerror = function(geneCVres){
  aveCVerror = 0
  for(i in seq_len(length(geneCVres))){
    aveCVerror = aveCVerror + geneCVres[[i]]$cvError
  }
  return(aveCVerror / length(geneCVres))
}

#### Function to create heatmaps from first function
myMap = function(cvRes, expData, titleIn = "", occur = 1){
  plotGenes = NULL
  
  for(i in seq_len(length(cvRes))){
    plotGenes = c(plotGenes, 
              names(table(unlist(cvRes[[i]]$genes)))[table(unlist(cvRes[[i]]$genes)) >= occur])
  }
  plotGenes = names(table(plotGenes))

  eSet.t = exprs(expData)
  label = cvRes[[1]]$label
  labLev = levels(label)
  
  ### Imputation
  imputedV = preProcess(t(eSet.t), method = cvRes[[1]]$impMethod)
  eSet = t(predict(imputedV, t(eSet.t)))
  
  dists = dist(t(eSet))
  cols <- colorRampPalette(rev(brewer.pal(9,"RdBu")))(255)
  distmat <- as.matrix(dists)
  df <- data.frame(condition=label,
                   row.names=colnames(distmat))
  
  eSet.plot = eSet[gsub('-', '.', rownames(expData)) %in% plotGenes, ]

  pheatmap(eSet.plot, color=cols,
           annotation_col=df, main = titleIn,
           show_rownames=FALSE, show_colnames=FALSE)
  }



data(GSE1827_eset)
a1 = GSE1827_eset

# Top p-values
ss.p1.rf = geneCV(a1, pData(a1)$summarystage)
ss.p1.glmb = geneCV(a1, pData(a1)$summarystage, fitMethod = "glmboost")

ss.p2.rf = geneCV(a1, pData(a1)$summarystage, pCut = 0.00001)
ss.p2.glmb = geneCV(a1, pData(a1)$summarystage, pCut = 0.00001, fitMethod = "glmboost")

### Most Significant
ss.ns1.rf = geneCV(a1, pData(a1)$summarystage, nSelect = 50)
ss.ns1.glmb = geneCV(a1, pData(a1)$summarystage, nSelect = 50, fitMethod = "glmboost")

# Randomly Selected
ss.rs1.rf = geneCV(a1, pData(a1)$summarystage, nSelect = 50, randomFlag = TRUE)
ss.rs1.glmb = geneCV(a1, pData(a1)$summarystage, nSelect = 50,
                     randomFlag = TRUE, fitMethod = "glmboost")

ss.p1.svm = geneCV(a1, pData(a1)$summarystage, fitMethod = "svmLinearWeights")
ss.p2.svm = geneCV(a1, pData(a1)$summarystage, pCut = 0.00001, fitMethod = "svmLinearWeights")
ss.ns1.svm = geneCV(a1, pData(a1)$summarystage, nSelect = 50, fitMethod = "svmLinearWeights")
ss.rs1.svm = geneCV(a1, pData(a1)$summarystage, nSelect = 50, 
                    randomFlag = TRUE, fitMethod = "svmLinearWeights")

resMat = matrix(NA, nrow = 3, ncol = 4)

resMat[1, 1] = aveCVerror(ss.p1.rf)
resMat[1, 2] = aveCVerror(ss.p2.rf)
resMat[1, 3] = aveCVerror(ss.ns1.rf)
resMat[1, 4] = aveCVerror(ss.rs1.rf)
resMat[2, 1] = aveCVerror(ss.p1.glmb)
resMat[2, 2] = aveCVerror(ss.p2.glmb)
resMat[2, 3] = aveCVerror(ss.ns1.glmb)
resMat[2, 4] = aveCVerror(ss.rs1.glmb)
resMat[3, 1] = aveCVerror(ss.p1.svm)
resMat[3, 2] = aveCVerror(ss.p2.svm)
resMat[3, 3] = aveCVerror(ss.ns1.svm)
resMat[3, 4] = aveCVerror(ss.rs1.svm)

colnames(resMat) = c("p<0.000001", "p<0.00001", "Top 50", "Random 50")
rownames(resMat) = c("Random Forest", "GLM Boost", "SVM")
```

To try and determine different genes that are important in distinguishing the difference between the superficial and muscle-invasive tumors, we use the cross-validation procedure described above to find certain sets of genes. With 5-fold cross-validation repeated 10 times, we have 10 potential sets of "important" genes. Within each run, we can select genes that are considered differntially expressed once, twice, or any number up to all five times. 

We try this below and see some interesting items. It should be noted that in these plots, there may be some genes with extreme values, which will cause the general pattern to become faded. This flaw unfortunately was unable to be addressed at this time.

Firstly, we look at the sets of genes selected as differntially expressed with a p-value cutoff of 0.000001. We consider three different subsets of the selected genes based upon the number of times the gene was considered differntially expressed: 1, 2, 5. It should be mentioned again that occurring 5 times means that any gene that was included in all 5 folds of any of the 10 repetitions will be included.

```{r heatmaps1, echo = FALSE, message = FALSE, eval=FALSE}
myMap(ss.p1.rf, a1, titleIn = "Differentially Expressed Genes (p < 0.00001) - Occur Once", occur = 1)
myMap(ss.p1.rf, a1, titleIn = "Differentially Expressed Genes (p < 0.00001) - Occur Twice", occur = 2)
myMap(ss.p1.rf, a1, titleIn = "Differentially Expressed Genes (p < 0.00001) - Occurs in All", occur = 5)
```

We notice that a more distinct patter seems to appear as we limit the number of genes that are included. This may be due to some genes simply being noise in the larger sets, or may also be due to some washing out of gene expression levels due to higher scales of some included genes (as can be seen in the legend). 

We look further and consider a couple of graphs where the cutoff for the p-value is not as extreme. 

```{r heatmaps2, echo = FALSE, message = FALSE, eval=FALSE}
myMap(ss.p2.rf, a1, titleIn = "Differentially Expressed Genes (p < 0.0001) - Occur Once", occur = 1)
myMap(ss.p2.rf, a1, titleIn = "Differentially Expressed Genes (p < 0.0001) - Occurs in All", occur = 5)
```

Here we again see a similar story to what was seen with the more extreme p-value cut-off. 

Finally, we consider a random selection of genes to see if we see a distinct pattern between the two tumor types. If this is the case, then we either would need to conclude that all of the genes are impacted by the tumor type (which may be indicative of a potential batch effect), or that even a random sample is good at differentiating between the two tumor types. We randomly select 50 genes for this map. For a good comparison, we select the 50 "most significant" differntially expressed genes and create another heatmap to see if there is a difference between the two. In either, we use any gene selected in any fold, giving us ~250 randomly selected genes in the random heat map (possibly less if the same gene was selected in more than one fold).

```{r heatmaps3, echo = FALSE, message = FALSE, eval=FALSE}
myMap(ss.rs1.rf, a1, titleIn = "50 Randomly Selected Genes", occur = 1)
myMap(ss.ns1.rf, a1, titleIn = "50 Most Significant Genes - Occurs Once", occur = 1)
```

We do not see a clear distinction in the randomly selected genes. Rather, we see a fairly even color scheme across all subjects. However, we do see a good distinction in the most significant genes. Thus, it does appear that modeling and the analysis are worthwhile in this pursuit.

# Cross-Validation Errors

Finally, we consider the predictive ability of several methods using several approaches to selecting genes. Here, we use Random Forests, GLM Boosting, and Support Vector Machines. Within these, we use a selection procedure as used above on the heatmaps to determine which genes should be used in the model. Namely, we select all genes in a fold considered differntially expressed (p-value less than 0.000001, and 0.00001), the 50 genes with the smallest p-values, and 50 randomly sampled genes. We can see in Table \ref{tab:cvRes} the results. 

```{r cvRes, echo = FALSE, eval=FALSE}
library(knitr)
kable(resMat, caption = "\\label{tab:cvRes}CV Results", align ='c')
```

We see the best results using Random Forests with the slightly larger set of genes (p < 0.00001). Random Forests does perform the best with the smallest set of genes (Top 50), and all methods perform worst using the random draw. However, we do see fairly remarkable accuracy with the random draw, which might again indicate a potential batch effect in the data. Or it may be that a single gene that is differntially expressed is randomly included and does manage to provide enough information for these methods to make some distinction. 

Of additional note, GLM Boosting appears to perform similarly well across all selections of related genes, SVM performs better with a larger set of genes, and performs the worst with the random draw.

# Conclusion

Thus, we can see that many methods exist that can be used to determine differences between superficial and muscle-invasive tumors. While remarkable, the random sampling method does not perform as well as other methods, indicating that information is available in the gene expressions, and differences do exist between the tumor types. 





